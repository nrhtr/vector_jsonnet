data_dir = "/var/lib/vector"

[sources.apache_logs]
  ignore_older = 86400
  include = ["/var/log/apache2/*.log"]
  type = "file"

[transforms.apache_parser]
  inputs = ["apache_logs"]
  regex = "^(?P<host>[w.]+) - (?P<user>[w]+) (?P<bytes_in>[d]+) [(?P<timestamp>.*)] \"(?P<method>[w]+) (?P<path>.*)\" (?P<status>[d]+) (?P<bytes_out>[d]+)$"
  type = "regex_parser"

[transforms.apache_sampler]
  inputs = ["apache_parser"]
  rate = 50
  type = "sampler"

[transforms.log_to_metric]
  inputs = ["apache_parser"]
  type = "log_to_metric"
  [[transforms.log_to_metric.metrics]]
    field = "message"
    type = "counter"
  [[transforms.log_to_metric.metrics]]
    field = "bytes_out"
    increment_by_value = true
    name = "bytes_out_total"
    type = "counter"
  [[transforms.log_to_metric.metrics]]
    field = "bytes_out"
    type = "gauge"
  [[transforms.log_to_metric.metrics]]
    field = "user"
    type = "set"
  [[transforms.log_to_metric.metrics]]
    field = "bytes_out"
    name = "bytes_out_histogram"
    type = "histogram"

[sinks.es_cluster]
  host = "http://79.12.221.222:9200"
  index = "vector-%Y-%m-%d"
  inputs = ["apache_sampler"]
  type = "elasticsearch"

[sinks.prometheus]
  buckets = [0, 10, 100, 1000, 10000, 100001]
  inputs = ["log_to_metric"]
  namespace = "vector"
  type = "prometheus"

[sinks.s3_archives]
  [sinks.s3_archives.batch]
    max_size = 10000000
  bucket = "my-log-archives"
  compression = "gzip"
  encoding = "ndjson"
  inputs = ["apache_parser"]
  key_prefix = "date=%Y-%m-%d"
  region = "us-east-1"
  type = "aws_s3"


